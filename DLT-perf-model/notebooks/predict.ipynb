{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from itertools import count\n",
    "from typing import List, Dict\n",
    "from typing import Tuple, Any\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import MSELoss, LSTM, GRU, RNN, L1Loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import init_logging\n",
    "from base_module import MModule\n",
    "from data import MDataset, Graph, load_graphs\n",
    "from importlib import reload\n",
    "from config import Config\n",
    "import config\n",
    "from data import MDataset, Graph, GraphNode, load_graphs, save_dataset_pkl, load_dataset_pkl, save_scalers_pkl, load_scalers_pkl\n",
    "import data\n",
    "from base_module import MModule, pad_np_vectors\n",
    "import base_module\n",
    "from executor import single_train_loop, nested_detach, grid_search_loop\n",
    "import executor\n",
    "from objects import ModelType\n",
    "import objects\n",
    "from metric import MetricUtil\n",
    "import metric\n",
    "from logger import init_logging, logging\n",
    "import logger\n",
    "import gcn\n",
    "from gcn import GCNLayer\n",
    "import transformer\n",
    "from transformer import TransformerModel\n",
    "reload(config)\n",
    "reload(data)\n",
    "reload(base_module)\n",
    "reload(executor)\n",
    "reload(objects)\n",
    "reload(metric)\n",
    "reload(logger)\n",
    "reload(gcn)\n",
    "reload(transformer)\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_environment_str = \"T4_CPUALL\"\n",
    "normalizer_cls = StandardScaler  # MinMaxScaler\n",
    "batch_size = 128\n",
    "method_prefix = \"SubgraphBased\"\n",
    "scalers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_features(graph: Graph, subgraph_node_size: int = 10, step: int = 5, dataset_params: Dict = {}) -> \\\n",
    "        Tuple[List[Dict], List[Dict]]:\n",
    "    subgraphs, _ = graph.subgraphs(\n",
    "        subgraph_node_size=subgraph_node_size, step=step)\n",
    "    X, Y = list(), list()\n",
    "\n",
    "    def subgraph_feature(nodes: List[GraphNode]):\n",
    "        feature_matrix = list()\n",
    "        for node in nodes:\n",
    "            feature = node.op.to_feature_array(\n",
    "                mode=dataset_params.get(\"mode\", \"complex\"))\n",
    "            feature = np.array(feature)\n",
    "            feature_matrix.append(feature)\n",
    "\n",
    "        feature_matrix = pad_np_vectors(feature_matrix)\n",
    "        feature_matrix = np.array(feature_matrix)\n",
    "\n",
    "        adj_matrix = [\n",
    "            [0.] * len(nodes) for _ in range(len(nodes))\n",
    "        ]\n",
    "        for curr_idx, node in enumerate(nodes):\n",
    "            if curr_idx + 1 < len(nodes):\n",
    "                adj_matrix[curr_idx][curr_idx+1] = 1.\n",
    "\n",
    "        adj_matrix = np.array(adj_matrix)\n",
    "        # x\n",
    "        feature = {\n",
    "            \"x_graph_id\": graph.ID,\n",
    "            \"x_node_ids\": \"|\".join([str(node.node_id) for node in nodes]),\n",
    "            \"x_subgraph_feature\": feature_matrix,\n",
    "            \"x_adj_matrix\": adj_matrix\n",
    "        }\n",
    "\n",
    "        # y\n",
    "        subgraph_duration = sum(node.duration + node.gap for node in subgraph)\n",
    "        nodes_durations = list()\n",
    "        for node in subgraph:\n",
    "            node_duration_label = (\n",
    "                node.duration, node.gap\n",
    "            )\n",
    "            nodes_durations.append(node_duration_label)\n",
    "\n",
    "        label = {\n",
    "            \"y_graph_id\": graph.ID,\n",
    "            \"y_nodes_durations\": nodes_durations,\n",
    "            \"y_subgraph_durations\": (subgraph_duration,)\n",
    "        }\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    for i, subgraph in enumerate(subgraphs):\n",
    "        x, y = subgraph_feature(subgraph)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def init_dataset(graphs: List[Graph]) -> MDataset:\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    subgraph_feature_maxsize = 0\n",
    "\n",
    "    for graph in graphs:\n",
    "        X_, Y_ = subgraph_features(graph=graph,\n",
    "                                   subgraph_node_size=12,\n",
    "                                   step=3,\n",
    "                                   dataset_params={\n",
    "                                       \"duration_summed\": False\n",
    "                                   })\n",
    "        for x in X_:\n",
    "            subgraph_feature_size = len(x[\"x_subgraph_feature\"][0])\n",
    "            subgraph_feature_maxsize = max(\n",
    "                subgraph_feature_maxsize, subgraph_feature_size)\n",
    "\n",
    "        X.extend(X_)\n",
    "        Y.extend(Y_)\n",
    "\n",
    "    for x in X:\n",
    "        x[\"x_subgraph_feature\"] = pad_np_vectors(\n",
    "            x[\"x_subgraph_feature\"], maxsize=subgraph_feature_maxsize)\n",
    "\n",
    "    dataset = MDataset(X, Y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(ds: MDataset, scalers) -> MDataset:\n",
    "    x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "\n",
    "    processed_features = list()\n",
    "    processed_labels = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "        assert isinstance(x_subgraph_feature, list)\n",
    "        x_subgraph_feature = np.array(x_subgraph_feature).astype(np.float32)\n",
    "        transformed_x_subgraph_feature = x_subgraph_feature_scaler.transform(\n",
    "            x_subgraph_feature)\n",
    "\n",
    "        x_adj_matrix = feature[\"x_adj_matrix\"]\n",
    "        x_adj_matrix = np.array(x_adj_matrix).astype(np.float32)\n",
    "\n",
    "        y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "        assert isinstance(y_nodes_durations, list)\n",
    "        y_nodes_durations = np.array(y_nodes_durations).astype(np.float32)\n",
    "        transformed_y_nodes_durations = y_nodes_durations_scaler.transform(\n",
    "            y_nodes_durations)\n",
    "\n",
    "        y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "        y_subgraph_durations_array = (y_subgraph_durations,)\n",
    "        y_subgraph_durations_array = y_subgraph_durations_scaler.transform(\n",
    "            y_subgraph_durations_array)\n",
    "        transformed_y_subgraph_durations = y_subgraph_durations_array[0]\n",
    "\n",
    "        processed_features.append({\n",
    "            \"x_graph_id\": feature[\"x_graph_id\"],\n",
    "            \"x_node_ids\": feature[\"x_node_ids\"],\n",
    "            \"x_subgraph_feature\": torch.Tensor(transformed_x_subgraph_feature),\n",
    "            \"x_adj_matrix\": torch.Tensor(x_adj_matrix)\n",
    "        })\n",
    "\n",
    "        processed_labels.append({\n",
    "            \"y_graph_id\": label[\"y_graph_id\"],\n",
    "            \"y_nodes_durations\": torch.Tensor(transformed_y_nodes_durations),\n",
    "            \"y_subgraph_durations\": torch.Tensor(transformed_y_subgraph_durations)\n",
    "        })\n",
    "\n",
    "    ds = MDataset(processed_features, processed_labels)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(MModule):\n",
    "    def __init__(self, feature_size, nodes_durations_len, num_layers, bidirectional, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = LSTM(input_size=feature_size, hidden_size=feature_size, num_layers=num_layers, batch_first=True,\n",
    "                         bidirectional=bidirectional)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.project = torch.nn.Linear(\n",
    "            in_features=feature_size * num_directions, out_features=nodes_durations_len)\n",
    "        self.loss_fn = L1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_search_model_params() -> Dict[str, List[Any]]:\n",
    "        return {\n",
    "            \"num_layers\": [4, 6, 8],\n",
    "            \"bidirectional\": [True, False],\n",
    "            \"learning_rate\": [1e-4, 1e-5],\n",
    "            'batch_size': [32, 64],\n",
    "            'epochs': [20],\n",
    "            'optimizer': ['Adam', 'SGD'],\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_subgraph_feature\"]\n",
    "        out, _ = self.lstm(X)\n",
    "        Y = self.project(out)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_nodes_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(device, features, labels):\n",
    "    features['x_subgraph_feature'] = features['x_subgraph_feature'].to(\n",
    "        device)\n",
    "    features['x_adj_matrix'] = features['x_adj_matrix'].to(device)\n",
    "    labels['y_nodes_durations'] = labels['y_nodes_durations'].to(device)\n",
    "    labels['y_subgraph_durations'] = labels['y_subgraph_durations'].to(\n",
    "        device)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_nodes_durations(outputs_, node_ids_str_, scalers):\n",
    "    x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "    node_to_durations = defaultdict(list)\n",
    "    for i, output_ in enumerate(outputs_):\n",
    "        node_ids = node_ids_str_[i]\n",
    "        node_ids_ = node_ids.split(\"|\")\n",
    "        assert len(output_) == len(node_ids_)\n",
    "        transformed: np.ndarray = y_nodes_durations_scaler.inverse_transform(\n",
    "            output_)\n",
    "        for i, node_id in enumerate(node_ids_):\n",
    "            node_to_durations[node_id].append(np.sum(transformed[i]))\n",
    "    node_to_duration = {k: np.average(v)\n",
    "                        for k, v in node_to_durations.items()}\n",
    "    return node_to_duration\n",
    "\n",
    "\n",
    "def compute_durations(input_batches, output_batches, scalers, eval_graphs):\n",
    "    graph_id_to_node_to_duration = defaultdict(lambda: defaultdict(list))\n",
    "    for inputs, outputs in zip(input_batches, output_batches):\n",
    "        outputs = nested_detach(outputs)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        graph_groups = defaultdict(list)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            graph_groups[graph_id].append(i)\n",
    "\n",
    "        for graph_id, indices in graph_groups.items():\n",
    "            group_x_node_ids = [v for i, v in enumerate(\n",
    "                inputs[\"x_node_ids\"]) if i in indices]\n",
    "            group_outputs = [v for i, v in enumerate(outputs) if i in indices]\n",
    "            node_to_durations = compute_graph_nodes_durations(\n",
    "                group_outputs, group_x_node_ids, scalers)\n",
    "            for node, duration in node_to_durations.items():\n",
    "                graph_id_to_node_to_duration[graph_id][node].append(duration)\n",
    "    graph_id_to_duration_pred = dict()\n",
    "    # average\n",
    "    for graph_id, node_to_duration in graph_id_to_node_to_duration.items():\n",
    "        duration_pred = 0\n",
    "        for _, duration_preds in node_to_duration.items():\n",
    "            duration_pred += np.average(duration_preds)\n",
    "        graph_id_to_duration_pred[graph_id] = duration_pred\n",
    "\n",
    "    # get y and y_pred\n",
    "    y_hat, y = list(), list()\n",
    "    for graph in eval_graphs:\n",
    "        pred = graph_id_to_duration_pred[graph.ID]\n",
    "        ground_truth = graph.graph_duration\n",
    "        y_hat.append(pred)\n",
    "        y.append(ground_truth)\n",
    "    return np.array(y_hat), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_size = 200000\n",
    "models = {\n",
    "    'T4_CPUALL': '/root/guohao/DLT-perf-model/notebooks/ckpts/subgraph/T4_CPUALL/LSTM/single_train2024-01-08_11-12-26/ckpt_255000.pth',\n",
    "    'P4_CPUALL': '/root/guohao/DLT-perf-model/notebooks/ckpts/subgraph/P4_CPUALL/LSTM/single_train2024-01-11_09-06-17/ckpt_160000.pth',\n",
    "    'RTX2080Ti_CPUALL': '/root/guohao/DLT-perf-model/notebooks/ckpts/subgraph/RTX2080Ti_CPUALL/LSTM/single_train2023-12-21_17-13-15/ckpt_255000.pth',\n",
    "    'RTX3080Ti_CPUALL': '/root/guohao/DLT-perf-model/notebooks/ckpts/subgraph/RTX3080Ti_CPUALL/LSTM/single_train2024-01-10_02-48-43/ckpt_255000.pth',\n",
    "}\n",
    "res = []\n",
    "for dataset_environment_str, model_ckpt in models.items():\n",
    "    eval_graphs = load_graphs(dataset_environment_str,\n",
    "                              train_or_eval=\"eval\",\n",
    "                              use_dummy=False,\n",
    "                              max_row=eval_size)\n",
    "\n",
    "    eval_ds = init_dataset(eval_graphs)\n",
    "    scalers = load_scalers_pkl(dataset_environment_str, method_prefix, 'train',\n",
    "                               'Standard')\n",
    "    preprocessed_eval_ds = preprocess_dataset(eval_ds, scalers)\n",
    "    model = torch.load(model_ckpt)\n",
    "    model.eval()\n",
    "    ds = preprocessed_eval_ds\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    input_batchs, output_batchs = list(), list()\n",
    "    for data in dl:\n",
    "        features, labels = data\n",
    "        features, labels = to_device(\"cuda\", features, labels)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "        input_batchs.append(features)\n",
    "        output_batchs.append(outputs)\n",
    "    y_hat, y = compute_durations(\n",
    "        input_batchs, output_batchs, scalers, eval_graphs)\n",
    "    rand_y, rand_y_hat = [], []\n",
    "    models_y, models_y_hat = [], []\n",
    "    for i in range(len(eval_graphs)):\n",
    "        graph = eval_graphs[i]\n",
    "        if 'rand' in graph.ID:\n",
    "            rand_y.append(y[i])\n",
    "            rand_y_hat.append(y_hat[i])\n",
    "        else:\n",
    "            models_y.append(y[i])\n",
    "            models_y_hat.append(y_hat[i])\n",
    "    res.append(\n",
    "        {\n",
    "            'dataset': dataset_environment_str,\n",
    "            'mre': MetricUtil.mre(y, y_hat),\n",
    "            'rmse': MetricUtil.rmse(y, y_hat),\n",
    "            'rand_mre': MetricUtil.mre(rand_y, rand_y_hat),\n",
    "            'rand_rmse': MetricUtil.rmse(rand_y, rand_y_hat),\n",
    "            'models_mre': MetricUtil.mre(models_y, models_y_hat),\n",
    "            'models_rmse': MetricUtil.rmse(models_y, models_y_hat),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.DataFrame(res)\n",
    "df.to_csv(os.path.join('/root/guohao/DLT-perf-model/notebooks/exp/total_compare',\n",
    "          f'subgraph.result.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predict(y, y_hat):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(y)):\n",
    "        data.append([y[i]/1000, y_hat[i]/1000])\n",
    "\n",
    "    # 线性拟合计算参数\n",
    "    SumXiYi = 0\n",
    "    SumXi = 0\n",
    "    SumYi = 0\n",
    "    SumXi2 = 0\n",
    "    PointX = []\n",
    "    PointY = []\n",
    "    for item in range(len(data)):\n",
    "        XiYi = data[item][0] * data[item][1]\n",
    "        SumXiYi += XiYi\n",
    "        SumXi += data[item][0]\n",
    "        SumYi += data[item][1]\n",
    "        SumXi2 += data[item][0] * data[item][0]\n",
    "        PointX.append(data[item][0])\n",
    "        PointY.append(data[item][1])\n",
    "\n",
    "    w = (len(data) * SumXiYi - SumXi * SumYi) / \\\n",
    "        (len(data) * SumXi2 - SumXi * SumXi)\n",
    "    b = (SumXi2 * SumYi - SumXiYi * SumXi) / \\\n",
    "        (len(data) * SumXi2 - SumXi * SumXi)\n",
    "    w = round(w, 2)\n",
    "    b = round(b, 2)\n",
    "\n",
    "    X = np.arange(0, 2000, 0.1)\n",
    "    Y = w * X + b\n",
    "\n",
    "    plt.plot(X, Y, color='red')\n",
    "    plt.scatter(PointX, PointY, s=1,  color='blue')  # 散点图\n",
    "    plt.xlabel('measured time (ms)')\n",
    "    plt.ylabel('predicted time (ms)')\n",
    "\n",
    "    fig = plt.figure()  # 务必保留此行，设置绘图对象\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return w, b, fig  # 务必按此顺序返回\n",
    "\n",
    "\n",
    "y_, y_hat_ = [], []\n",
    "for i in range(len(y)):\n",
    "    if y[i] < 2000000 and y_hat[i] < 2000000:\n",
    "        y_.append(y[i])\n",
    "        y_hat_.append(y_hat[i])\n",
    "print(len(y_), len(y_hat_))\n",
    "plot_predict(y_, y_hat_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT-pref-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
