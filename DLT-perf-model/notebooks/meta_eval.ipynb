{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from itertools import count\n",
    "from typing import List, Dict\n",
    "from typing import Tuple, Any\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import MSELoss, LSTM, GRU, RNN, L1Loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import init_logging\n",
    "from base_module import MModule\n",
    "from data import MDataset, Graph, load_graphs\n",
    "from importlib import reload\n",
    "from config import Config\n",
    "import config\n",
    "from data import MDataset, Graph, GraphNode, load_graphs, save_dataset_pkl, load_dataset_pkl, save_scalers_pkl, load_scalers_pkl\n",
    "import data\n",
    "from base_module import MModule, pad_np_vectors\n",
    "import base_module\n",
    "from executor import single_train_loop, nested_detach, grid_search_loop\n",
    "import executor\n",
    "from objects import ModelType\n",
    "import objects\n",
    "from metric import MetricUtil\n",
    "import metric\n",
    "from logger import init_logging, logging\n",
    "import logger\n",
    "import gcn\n",
    "from gcn import GCNLayer\n",
    "import transformer\n",
    "from transformer import TransformerModel\n",
    "reload(config)\n",
    "reload(data)\n",
    "reload(base_module)\n",
    "reload(executor)\n",
    "reload(objects)\n",
    "reload(metric)\n",
    "reload(logger)\n",
    "reload(gcn)\n",
    "reload(transformer)\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_environment_str = \"T4_CPUALL\"\n",
    "scaler_cls = StandardScaler  # MinMaxScaler\n",
    "dummy = False\n",
    "model_type = ModelType.LSTM\n",
    "method_prefix = \"SubgraphBased\"\n",
    "eval_size = 200_000\n",
    "train_size = 10000\n",
    "epoch = 100\n",
    "eval_steps = 3000\n",
    "meta_model_path = 'ckpts/meta/LSTM/meta_train2024-01-04_17-19-49/ckpt_900.pth'\n",
    "scalers = None\n",
    "eval_graphs = None\n",
    "# 配置\n",
    "train_sizes = [10000, 50000, 100000, 150000, 200000, 250000, 300000]\n",
    "epoches = [300, 200, 100, 75, 75, 75, 75]\n",
    "eval_steps = [2000, 2000, 3000, 3000, 5000, 5000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalers(raw_train_ds: MDataset):\n",
    "\n",
    "    def _preprocess_required_data(ds: MDataset):\n",
    "        x_subgraph_feature_array = list()\n",
    "        y_nodes_durations_array = list()\n",
    "        y_subgraph_durations_array = list()\n",
    "\n",
    "        for data in ds:\n",
    "            feature, label = data\n",
    "            x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "            assert isinstance(x_subgraph_feature, list)\n",
    "            x_subgraph_feature_array.extend(x_subgraph_feature)\n",
    "\n",
    "            y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "            assert isinstance(y_nodes_durations, list)\n",
    "            y_nodes_durations_array.extend(y_nodes_durations)\n",
    "\n",
    "            y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "            y_subgraph_durations_array.append(y_subgraph_durations)\n",
    "\n",
    "        x_subgraph_feature_array = np.array(x_subgraph_feature_array)\n",
    "        y_nodes_durations_array = np.array(y_nodes_durations_array)\n",
    "        y_subgraph_durations_array = np.array(y_subgraph_durations_array)\n",
    "        return [x_subgraph_feature_array, y_nodes_durations_array, y_subgraph_durations_array]\n",
    "\n",
    "    x_subgraph_feature_array, y_nodes_durations_array, y_subgraph_durations_array = _preprocess_required_data(\n",
    "        ds=raw_train_ds)\n",
    "\n",
    "    x_subgraph_feature_scaler = scaler_cls()\n",
    "    x_subgraph_feature_scaler.fit(x_subgraph_feature_array)\n",
    "\n",
    "    y_nodes_durations_scaler = scaler_cls()\n",
    "    y_nodes_durations_scaler.fit(y_nodes_durations_array)\n",
    "\n",
    "    y_subgraph_durations_scaler = scaler_cls()\n",
    "    y_subgraph_durations_scaler.fit(y_subgraph_durations_array)\n",
    "\n",
    "    return x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler\n",
    "\n",
    "\n",
    "def subgraph_features(graph: Graph, subgraph_node_size: int = 10, step: int = 5, dataset_params: Dict = {}) -> \\\n",
    "        Tuple[List[Dict], List[Dict]]:\n",
    "    subgraphs, _ = graph.subgraphs(\n",
    "        subgraph_node_size=subgraph_node_size, step=step)\n",
    "    X, Y = list(), list()\n",
    "\n",
    "    def subgraph_feature(nodes: List[GraphNode]):\n",
    "        feature_matrix = list()\n",
    "        for node in nodes:\n",
    "            feature = node.op.to_feature_array(\n",
    "                mode=dataset_params.get(\"mode\", \"complex\"))\n",
    "            feature = np.array(feature)\n",
    "            feature_matrix.append(feature)\n",
    "\n",
    "        feature_matrix = pad_np_vectors(feature_matrix)\n",
    "        feature_matrix = np.array(feature_matrix)\n",
    "\n",
    "        adj_matrix = [\n",
    "            [0.] * len(nodes) for _ in range(len(nodes))\n",
    "        ]\n",
    "        for curr_idx, node in enumerate(nodes):\n",
    "            if curr_idx + 1 < len(nodes):\n",
    "                adj_matrix[curr_idx][curr_idx+1] = 1.\n",
    "\n",
    "        adj_matrix = np.array(adj_matrix)\n",
    "        # x\n",
    "        feature = {\n",
    "            \"x_graph_id\": graph.ID,\n",
    "            \"x_node_ids\": \"|\".join([str(node.node_id) for node in nodes]),\n",
    "            \"x_subgraph_feature\": feature_matrix,\n",
    "            \"x_adj_matrix\": adj_matrix\n",
    "        }\n",
    "\n",
    "        # y\n",
    "        subgraph_duration = sum(node.duration + node.gap for node in subgraph)\n",
    "        nodes_durations = list()\n",
    "        for node in subgraph:\n",
    "            node_duration_label = (\n",
    "                node.duration, node.gap\n",
    "            )\n",
    "            nodes_durations.append(node_duration_label)\n",
    "\n",
    "        label = {\n",
    "            \"y_graph_id\": graph.ID,\n",
    "            \"y_nodes_durations\": nodes_durations,\n",
    "            \"y_subgraph_durations\": (subgraph_duration,)\n",
    "        }\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    for i, subgraph in enumerate(subgraphs):\n",
    "        x, y = subgraph_feature(subgraph)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def init_dataset(graphs: List[Graph], conf) -> MDataset:\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    subgraph_feature_maxsize = 0\n",
    "\n",
    "    for graph in graphs:\n",
    "        X_, Y_ = subgraph_features(graph=graph,\n",
    "                                   subgraph_node_size=conf.dataset_subgraph_node_size,\n",
    "                                   step=conf.dataset_subgraph_step,\n",
    "                                   dataset_params=conf.dataset_params)\n",
    "        for x in X_:\n",
    "            subgraph_feature_size = len(x[\"x_subgraph_feature\"][0])\n",
    "            subgraph_feature_maxsize = max(\n",
    "                subgraph_feature_maxsize, subgraph_feature_size)\n",
    "\n",
    "        X.extend(X_)\n",
    "        Y.extend(Y_)\n",
    "\n",
    "    for x in X:\n",
    "        x[\"x_subgraph_feature\"] = pad_np_vectors(\n",
    "            x[\"x_subgraph_feature\"], maxsize=subgraph_feature_maxsize)\n",
    "\n",
    "    dataset = MDataset(X, Y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(ds: MDataset, scalers) -> MDataset:\n",
    "    x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "\n",
    "    processed_features = list()\n",
    "    processed_labels = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "        assert isinstance(x_subgraph_feature, list)\n",
    "        x_subgraph_feature = np.array(x_subgraph_feature).astype(np.float32)\n",
    "        transformed_x_subgraph_feature = x_subgraph_feature_scaler.transform(\n",
    "            x_subgraph_feature)\n",
    "\n",
    "        x_adj_matrix = feature[\"x_adj_matrix\"]\n",
    "        x_adj_matrix = np.array(x_adj_matrix).astype(np.float32)\n",
    "\n",
    "        y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "        assert isinstance(y_nodes_durations, list)\n",
    "        y_nodes_durations = np.array(y_nodes_durations).astype(np.float32)\n",
    "        transformed_y_nodes_durations = y_nodes_durations_scaler.transform(\n",
    "            y_nodes_durations)\n",
    "\n",
    "        y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "        y_subgraph_durations_array = (y_subgraph_durations,)\n",
    "        y_subgraph_durations_array = y_subgraph_durations_scaler.transform(\n",
    "            y_subgraph_durations_array)\n",
    "        transformed_y_subgraph_durations = y_subgraph_durations_array[0]\n",
    "\n",
    "        processed_features.append({\n",
    "            \"x_graph_id\": feature[\"x_graph_id\"],\n",
    "            \"x_node_ids\": feature[\"x_node_ids\"],\n",
    "            \"x_subgraph_feature\": torch.Tensor(transformed_x_subgraph_feature),\n",
    "            \"x_adj_matrix\": torch.Tensor(x_adj_matrix)\n",
    "        })\n",
    "\n",
    "        processed_labels.append({\n",
    "            \"y_graph_id\": label[\"y_graph_id\"],\n",
    "            \"y_nodes_durations\": torch.Tensor(transformed_y_nodes_durations),\n",
    "            \"y_subgraph_durations\": torch.Tensor(transformed_y_subgraph_durations)\n",
    "        })\n",
    "\n",
    "    ds = MDataset(processed_features, processed_labels)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluate_metrics(input_batches, output_batches, eval_loss) -> Dict[str, float]:\n",
    "    def compute_graph_nodes_durations(outputs_, node_ids_str_):\n",
    "        x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "        node_to_durations = defaultdict(list)\n",
    "        for i, output_ in enumerate(outputs_):\n",
    "            node_ids = node_ids_str_[i]\n",
    "            node_ids_ = node_ids.split(\"|\")\n",
    "            assert len(output_) == len(node_ids_)\n",
    "            transformed: np.ndarray = y_nodes_durations_scaler.inverse_transform(\n",
    "                output_)\n",
    "            for i, node_id in enumerate(node_ids_):\n",
    "                node_to_durations[node_id].append(np.sum(transformed[i]))\n",
    "        node_to_duration = {k: np.average(v)\n",
    "                            for k, v in node_to_durations.items()}\n",
    "        return node_to_duration\n",
    "\n",
    "    graph_id_to_node_to_duration = defaultdict(lambda: defaultdict(list))\n",
    "    for inputs, outputs in zip(input_batches, output_batches):\n",
    "        outputs = nested_detach(outputs)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        graph_groups = defaultdict(list)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            graph_groups[graph_id].append(i)\n",
    "\n",
    "        for graph_id, indices in graph_groups.items():\n",
    "            group_x_node_ids = [v for i, v in enumerate(\n",
    "                inputs[\"x_node_ids\"]) if i in indices]\n",
    "            group_outputs = [v for i, v in enumerate(outputs) if i in indices]\n",
    "            node_to_durations = compute_graph_nodes_durations(\n",
    "                group_outputs, group_x_node_ids)\n",
    "            for node, duration in node_to_durations.items():\n",
    "                graph_id_to_node_to_duration[graph_id][node].append(duration)\n",
    "    graph_id_to_duration_pred = dict()\n",
    "    # TODO check this!!!\n",
    "    for graph_id, node_to_duration in graph_id_to_node_to_duration.items():\n",
    "        duration_pred = 0\n",
    "        for _, duration_preds in node_to_duration.items():\n",
    "            duration_pred += np.average(duration_preds)\n",
    "        graph_id_to_duration_pred[graph_id] = duration_pred\n",
    "    duration_metrics = MetricUtil.compute_duration_metrics(\n",
    "        eval_graphs, graph_id_to_duration_pred)\n",
    "    return {\"eval_loss\": eval_loss, **duration_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_device(conf: Config, features, labels):\n",
    "    features['x_subgraph_feature'] = features['x_subgraph_feature'].to(\n",
    "        conf.device)\n",
    "    features['x_adj_matrix'] = features['x_adj_matrix'].to(conf.device)\n",
    "    labels['y_nodes_durations'] = labels['y_nodes_durations'].to(conf.device)\n",
    "    labels['y_subgraph_durations'] = labels['y_subgraph_durations'].to(\n",
    "        conf.device)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(MModule):\n",
    "    def __init__(self, feature_size, nodes_durations_len, num_layers, bidirectional, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = LSTM(input_size=feature_size, hidden_size=feature_size, num_layers=num_layers, batch_first=True,\n",
    "                         bidirectional=bidirectional)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.project = torch.nn.Linear(\n",
    "            in_features=feature_size * num_directions, out_features=nodes_durations_len)\n",
    "        self.loss_fn = L1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_search_model_params() -> Dict[str, List[Any]]:\n",
    "        return {\n",
    "            \"num_layers\": [4, 6, 8],\n",
    "            \"bidirectional\": [True, False],\n",
    "            \"learning_rate\": [1e-4, 1e-5],\n",
    "            'batch_size': [32, 64],\n",
    "            'epochs': [20],\n",
    "            'optimizer': ['Adam', 'SGD'],\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_subgraph_feature\"]\n",
    "        out, _ = self.lstm(X)\n",
    "        Y = self.project(out)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_nodes_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_LSTM_model() -> MModule | Any:\n",
    "    def default_model_params() -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"num_layers\": 4,\n",
    "            \"bidirectional\": True,\n",
    "        }\n",
    "    x_node_feature_size = 66\n",
    "    y_nodes_durations_len = 2\n",
    "    print(x_node_feature_size, y_nodes_durations_len)\n",
    "    model_params = {}\n",
    "    final_params = default_model_params()\n",
    "    for k, v in final_params.items():\n",
    "        final_params[k] = model_params.get(k, v)\n",
    "    print(final_params)\n",
    "    return LSTMModel(\n",
    "        feature_size=x_node_feature_size,\n",
    "        nodes_durations_len=y_nodes_durations_len,\n",
    "        **final_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(eval_steps)):\n",
    "    train_size = train_sizes[i]\n",
    "    epoch = epoches[i]\n",
    "    eval_step = epoches[i]\n",
    "    conf = Config.from_dict({\n",
    "        \"model\": \"LSTM\",\n",
    "        \"dataset_environment_str\": dataset_environment_str,\n",
    "        \"meta_dataset_environment_strs\": [dataset_environment_str],\n",
    "        \"dataset_subgraph_node_size\": 12,\n",
    "        \"dataset_subgraph_step\": 3,\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"model_params\": {\n",
    "            \"num_layers\": 4,\n",
    "            \"bidirectional\": True\n",
    "        },\n",
    "        \"dataset_dummy\": False,\n",
    "        \"batch_size\": 128,\n",
    "        \"eval_steps\": eval_step,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": epoch,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"train_size\": train_size,\n",
    "        \"eval_size\": eval_size,\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [dataset_environment_str],\n",
    "            \"meta_dataset_eval_environment_strs\": [dataset_environment_str],\n",
    "        },\n",
    "    })\n",
    "\n",
    "    eval_graphs = load_graphs(dataset_environment_str,\n",
    "                              train_or_eval=\"eval\",\n",
    "                              use_dummy=dummy,\n",
    "                              max_row=eval_size)\n",
    "    train_graphs = load_graphs(dataset_environment_str,\n",
    "                               train_or_eval=\"train\",\n",
    "                               use_dummy=dummy,\n",
    "                               max_row=train_size)  # todo 500\n",
    "    print(len(train_graphs), len(eval_graphs))\n",
    "    train_ds = init_dataset(train_graphs, conf)\n",
    "    eval_ds = init_dataset(eval_graphs, conf)\n",
    "    scalers = get_scalers(train_ds)\n",
    "    preprocessed_train_ds = preprocess_dataset(train_ds, scalers)\n",
    "    preprocessed_eval_ds = preprocess_dataset(eval_ds, scalers)\n",
    "    # model = torch.load(meta_model_path)\n",
    "    model = init_LSTM_model()\n",
    "    print(model)\n",
    "    model_type = ModelType.LSTM\n",
    "    single_train_loop(model_type, conf, preprocessed_train_ds,\n",
    "                      preprocessed_eval_ds, model, compute_evaluate_metrics, to_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT-pref-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
