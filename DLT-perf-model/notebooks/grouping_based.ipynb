{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "from typing import Tuple, Any\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import init_logging\n",
    "from base_module import MModule\n",
    "from data import MDataset, Graph, load_graphs\n",
    "from importlib import reload\n",
    "from config import Config\n",
    "import config\n",
    "from data import MDataset, Graph, GraphNode, load_graphs, save_dataset_pkl, load_dataset_pkl, save_scalers_pkl, load_scalers_pkl\n",
    "import data\n",
    "from base_module import MModule, pad_np_vectors\n",
    "import base_module\n",
    "from executor import single_train_loop, nested_detach\n",
    "import executor\n",
    "from objects import ModelType\n",
    "import objects\n",
    "from metric import MetricUtil\n",
    "import metric\n",
    "from logger import init_logging, logging\n",
    "import logger\n",
    "import gcn\n",
    "from gcn import GCNLayer\n",
    "\n",
    "# reload(gcn)\n",
    "# reload(config)\n",
    "# reload(data)\n",
    "# reload(base_module)\n",
    "# reload(executor)\n",
    "# reload(objects)\n",
    "# reload(metric)\n",
    "# reload(logger)\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_environment_str = \"T4_CPUALL\"\n",
    "normalizer_cls = StandardScaler  # MinMaxScaler\n",
    "dummy = False\n",
    "model_type = ModelType.GCNGrouping\n",
    "method_prefix = \"GroupingBased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = None\n",
    "# op_feature_scaler, y_scaler = None, None\n",
    "eval_graphs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = {\n",
    "    ModelType.MLPTestGrouping.name: Config.from_dict({\n",
    "        \"model\": \"MLPTestGrouping\",\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_environment_str\": dataset_environment_str,\n",
    "        \"dataset_subgraph_grouping_count\": 20,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"dataset_dummy\": False,\n",
    "        \"batch_size\": 32,\n",
    "        \"eval_steps\": 5000,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [dataset_environment_str],\n",
    "            \"meta_dataset_eval_environment_strs\": [dataset_environment_str],\n",
    "        },\n",
    "    }),\n",
    "    ModelType.GCNGrouping.name: Config.from_dict({\n",
    "        \"model\": \"GCNGrouping\",\n",
    "        \"dataset_environment_str\": dataset_environment_str,\n",
    "        \"dataset_subgraph_grouping_count\": 25,\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"dataset_dummy\": False,\n",
    "        \"batch_size\": 32,\n",
    "        \"eval_steps\": 25,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [dataset_environment_str],\n",
    "            \"meta_dataset_eval_environment_strs\": [dataset_environment_str],\n",
    "        },\n",
    "    }),\n",
    "}\n",
    "\n",
    "conf: Config = train_configs[model_type.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_graph_feature(graph: Graph, subgraph_count: int = 10, dataset_params: Dict = {}) -> Tuple[\n",
    "        Dict[str, np.ndarray], Dict]:\n",
    "    subgraphs, node_id_to_group_idx = graph.subgraphs(\n",
    "        subgraph_count=subgraph_count)\n",
    "\n",
    "    feature_matrix = list()\n",
    "    for subgraph in subgraphs:\n",
    "        subgraph_features = list()\n",
    "        for node in subgraph:\n",
    "            node_feature = np.array(node.op.to_feature_array(\n",
    "                mode=dataset_params.get(\"mode\", \"complex\")))\n",
    "            subgraph_features.append(node_feature)\n",
    "        if len(subgraph_features) == 0:\n",
    "            feature_matrix.append(np.zeros(1))\n",
    "            continue\n",
    "        subgraph_features = pad_np_vectors(subgraph_features)\n",
    "        feature = np.sum(subgraph_features, axis=0)\n",
    "        feature = np.append(feature, len(subgraph))\n",
    "        feature_matrix.append(feature)\n",
    "    adj_matrix = [\n",
    "        [0.] * len(subgraphs) for _ in range(len(subgraphs))\n",
    "    ]\n",
    "    for curr_idx in range(len(subgraphs)):\n",
    "        if curr_idx + 1 < len(subgraphs):\n",
    "            adj_matrix[curr_idx][curr_idx+1] = 1.\n",
    "\n",
    "    adj_matrix = np.array(adj_matrix)\n",
    "\n",
    "    # adjacency_matrix = list()\n",
    "    # for i, subgraph in enumerate(subgraphs):\n",
    "    #     vector = np.zeros(len(subgraphs) + 1)\n",
    "    #     for node in subgraph:\n",
    "    #         neighbor_group_indices = list()\n",
    "    #         for neighbor in node.neighbors:\n",
    "    #             neighbor_group_idx = node_id_to_group_idx[neighbor.node_id]\n",
    "    #             if neighbor_group_idx != i:\n",
    "    #                 neighbor_group_indices.append(neighbor_group_idx)\n",
    "    #         for idx in neighbor_group_indices:\n",
    "    #             vector[idx] = 1\n",
    "    #     adjacency_matrix.append(vector)\n",
    "\n",
    "    feature_matrix = pad_np_vectors(feature_matrix)\n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    adj_matrix = np.array(adj_matrix)\n",
    "\n",
    "    x = {\n",
    "        \"x_graph_id\": graph.ID,\n",
    "        \"x_feature_matrix\": feature_matrix,\n",
    "        \"x_adjacency_matrix\": adj_matrix,\n",
    "    }\n",
    "    y = {\n",
    "        \"y_graph_id\": graph.ID,\n",
    "        \"y_graph_duration\": (graph.graph_duration,)\n",
    "    }\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def init_dataset(graphs: List[Graph]) -> MDataset:\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    feature_matrix_maxsize = 0\n",
    "    adjacency_matrix_maxsize = 0\n",
    "\n",
    "    for graph in graphs:\n",
    "        x, y = full_graph_feature(graph,\n",
    "                                  subgraph_count=conf.dataset_subgraph_grouping_count,\n",
    "                                  dataset_params=conf.dataset_params)\n",
    "        feature_matrix_size = len(x[\"x_feature_matrix\"][0])\n",
    "        adjacency_matrix_size = len(x[\"x_adjacency_matrix\"][0])\n",
    "        # print(f\"feature_matrix_size: {x['x_feature_matrix'].shape}, adjacency_matrix_size: {x['x_adjacency_matrix'].shape}\")\n",
    "        feature_matrix_maxsize = max(\n",
    "            feature_matrix_maxsize, feature_matrix_size)\n",
    "        adjacency_matrix_maxsize = max(\n",
    "            adjacency_matrix_maxsize, adjacency_matrix_size)\n",
    "\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    for x in X:\n",
    "        x[\"x_feature_matrix\"] = pad_np_vectors(\n",
    "            x[\"x_feature_matrix\"], maxsize=feature_matrix_maxsize)\n",
    "        x[\"x_adjacency_matrix\"] = pad_np_vectors(\n",
    "            x[\"x_adjacency_matrix\"], maxsize=adjacency_matrix_maxsize)\n",
    "\n",
    "    dataset = MDataset(X, Y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalers(ds: MDataset):\n",
    "    scaler_cls = conf.dataset_normalizer_cls\n",
    "    graph_feature_array = list()\n",
    "    y_array = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        x_feature_matrix = feature[\"x_feature_matrix\"]\n",
    "        assert isinstance(x_feature_matrix, list)\n",
    "        graph_feature_array.extend(x_feature_matrix)\n",
    "        y_array.append(label[\"y_graph_duration\"])\n",
    "\n",
    "    graph_feature_array = np.array(graph_feature_array)\n",
    "    y_array = np.array(y_array)\n",
    "\n",
    "    graph_feature_scaler = scaler_cls()\n",
    "    graph_feature_scaler.fit(graph_feature_array)\n",
    "\n",
    "    y_scaler = scaler_cls()\n",
    "    y_scaler.fit(y_array)\n",
    "    return graph_feature_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(ds: MDataset) -> MDataset:\n",
    "    y_array = list()\n",
    "\n",
    "    graph_feature_scaler, y_scaler = scalers\n",
    "    graph_feature_arrays = list()\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        # x. transform for each x feature matrix. do not merge them.\n",
    "        x_feature_matrix = feature[\"x_feature_matrix\"]\n",
    "        x_feature_matrix = np.array(x_feature_matrix, dtype=np.float32)\n",
    "\n",
    "        graph_feature_array = graph_feature_scaler.transform(x_feature_matrix)\n",
    "        graph_feature_arrays.append(graph_feature_array)\n",
    "        # y. transform altogether\n",
    "        y_array.append(label[\"y_graph_duration\"])\n",
    "\n",
    "    y_array = np.array(y_array, dtype=np.float32)\n",
    "    y_array = y_scaler.transform(y_array)\n",
    "\n",
    "    processed_features = list()\n",
    "    processed_labels = list()\n",
    "    for i, data in enumerate(ds):\n",
    "        feature, label = data\n",
    "        x_adjacency_matrix = np.array(\n",
    "            feature[\"x_adjacency_matrix\"], dtype=np.float32)\n",
    "        processed_features.append({\n",
    "            \"x_graph_id\": feature[\"x_graph_id\"],\n",
    "            \"x_feature_matrix\": torch.Tensor(graph_feature_arrays[i]),\n",
    "            \"x_adjacency_matrix\": torch.Tensor(x_adjacency_matrix)\n",
    "        })\n",
    "        # print(f\"x_feature_matrix: {graph_feature_arrays[i].shape}, x_adjacency_matrix: {x_adjacency_matrix.shape}\")\n",
    "        processed_labels.append({\n",
    "            \"y_graph_id\": label[\"y_graph_id\"],\n",
    "            \"y_graph_duration\": torch.Tensor(y_array[i]),\n",
    "        })\n",
    "\n",
    "    ds = MDataset(processed_features, processed_labels)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluate_metrics(input_batches, output_batches, eval_loss) -> Dict[str, float]:\n",
    "    batches_len = len(input_batches)\n",
    "\n",
    "    def compute_graph_duration(_logits):\n",
    "        _, y_scaler = scalers\n",
    "        transformed: np.ndarray = y_scaler.inverse_transform(_logits)\n",
    "        duration_dim = (0, 1)\n",
    "        durations = transformed[:, duration_dim[0]:duration_dim[1]].sum(axis=1)\n",
    "        return durations\n",
    "\n",
    "    graph_id_to_duration_pred = defaultdict(int)\n",
    "    for idx in range(batches_len):\n",
    "        inputs = input_batches[idx]\n",
    "        logits = output_batches[idx]\n",
    "        logits = nested_detach(logits)\n",
    "        logits = logits.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        graph_durations = compute_graph_duration(logits)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            graph_duration = graph_durations[i].item()\n",
    "            graph_id_to_duration_pred[graph_id] = graph_duration\n",
    "    duration_metrics = MetricUtil.compute_duration_metrics(\n",
    "        eval_graphs, graph_id_to_duration_pred)\n",
    "    return {\"eval_loss\": eval_loss, **duration_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_durations(input_batches, output_batches, scalers, eval_graphs) -> Dict[str, float]:\n",
    "\n",
    "    def compute_graph_duration(_logits):\n",
    "        _, y_scaler = scalers\n",
    "        transformed: np.ndarray = y_scaler.inverse_transform(_logits)\n",
    "        duration_dim = (0, 1)\n",
    "        durations = transformed[:, duration_dim[0]:duration_dim[1]].sum(axis=1)\n",
    "        return durations\n",
    "\n",
    "    batches_len = len(input_batches)\n",
    "    graph_id_to_duration_pred = defaultdict(int)\n",
    "    for idx in range(batches_len):\n",
    "        inputs = input_batches[idx]\n",
    "        logits = output_batches[idx]\n",
    "        logits = nested_detach(logits)\n",
    "        logits = logits.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        graph_durations = compute_graph_duration(logits)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            graph_duration = graph_durations[i].item()\n",
    "            graph_id_to_duration_pred[graph_id] = graph_duration\n",
    "\n",
    "    y_hat, y = list(), list()\n",
    "    for graph in eval_graphs:\n",
    "        y_hat.append(graph_id_to_duration_pred[graph.ID])\n",
    "        y.append(graph.graph_duration)\n",
    "    return np.array(y_hat), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_device(conf: Config, features, labels):\n",
    "    features[\"x_feature_matrix\"] = features[\"x_feature_matrix\"].to('cuda')\n",
    "    features[\"x_adjacency_matrix\"] = features[\"x_adjacency_matrix\"].to(\n",
    "        'cuda')\n",
    "    labels[\"y_graph_duration\"] = labels[\"y_graph_duration\"].to('cuda')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPTest_GroupingModel(MModule):\n",
    "\n",
    "    def __init__(self, input_shape, output_dimension, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(\n",
    "            in_features=input_shape[0] * input_shape[1], out_features=128)\n",
    "        self.output = torch.nn.Linear(128, output_dimension)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_feature_matrix\"]\n",
    "        X = self.flatten(X)\n",
    "        X = self.linear1(X)\n",
    "        Y = self.output(X)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        graph_duration = Y[\"y_graph_duration\"]\n",
    "        loss = self.loss_fn(outputs, graph_duration)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_MLPTestGrouping_model() -> MModule | Any:\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    shape = len(sample_x_dict[\"x_feature_matrix\"]), len(\n",
    "        sample_x_dict[\"x_feature_matrix\"][0])\n",
    "    return MLPTest_GroupingModel(input_shape=shape,\n",
    "                                 output_dimension=len(sample_y_dict[\"y_graph_duration\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GCNGroupingModel(MModule):\n",
    "    def __init__(self, dim_feats, dim_h, y_graph_duration_len, n_layers, dropout):\n",
    "        super(GCNGroupingModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GCNLayer(dim_feats, dim_h, F.relu, 0))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GCNLayer(dim_h, dim_h, F.relu, dropout))\n",
    "        # output layer\n",
    "        self.layers.append(\n",
    "            GCNLayer(dim_h, y_graph_duration_len, None, dropout))\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        adj, features = X[\"x_adjacency_matrix\"], X[\"x_feature_matrix\"]\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(adj, h)\n",
    "        graph_duration = torch.sum(h, dim=[1])\n",
    "        return graph_duration\n",
    "\n",
    "    def compute_loss(self, outputs, Y) -> torch.Tensor:\n",
    "        y_graph_duration = Y[\"y_graph_duration\"]\n",
    "        loss = self.loss_fn(outputs, y_graph_duration)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_GCNGrouping_model(preprocessed_train_ds) -> MModule | Any:\n",
    "    def default_model_params() -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"dim_h\": None,\n",
    "            \"n_layers\": 8,\n",
    "            \"dropout\": 0.01\n",
    "        }\n",
    "\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    x_node_feature_size = len(sample_x_dict[\"x_feature_matrix\"][0])\n",
    "    y_graph_duration_len = len(sample_y_dict[\"y_graph_duration\"])\n",
    "    model_params = conf.model_params\n",
    "\n",
    "    final_model_params = default_model_params()\n",
    "    default_dim_h = x_node_feature_size if final_model_params.get(\"dim_h\") is None else final_model_params.get(\n",
    "        \"dim_h\")\n",
    "    final_model_params[\"dim_h\"] = model_params.get(\"dim_h\", default_dim_h)\n",
    "    final_model_params[\"n_layers\"] = model_params.get(\n",
    "        \"n_layers\", final_model_params[\"n_layers\"])\n",
    "    final_model_params[\"dropout\"] = model_params.get(\n",
    "        \"dropout\", final_model_params[\"dropout\"])\n",
    "    return GCNGroupingModel(\n",
    "        dim_feats=x_node_feature_size,\n",
    "        y_graph_duration_len=y_graph_duration_len,\n",
    "        **final_model_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_process\n",
    "eval_size = 200_000\n",
    "batch_size = 32\n",
    "models = {\n",
    "    'T4_CPUALL': '/root/guohao/DLT-perf-model/notebooks/ckpts/grouping/T4_CPUALL/GCNGrouping/single_train2024-01-09_12-43-44/ckpt_2575.pth',\n",
    "    'P4_CPUALL': '/root/guohao/DLT-perf-model/notebooks/ckpts/grouping/P4_CPUALL/GCNGrouping/single_train2024-01-09_12-48-48/ckpt_1525.pth',\n",
    "    'RTX2080Ti_CPUALL': '/root/guohao/DLT-perf-model/notebooks/ckpts/grouping/RTX2080Ti_CPUALL/GCNGrouping/single_train2024-01-09_12-56-14/ckpt_2350.pth',\n",
    "    'RTX3080Ti_CPUALL': '/root/guohao/DLT-perf-model/notebooks/ckpts/grouping/RTX3080Ti_CPUALL/GCNGrouping/single_train2024-01-10_01-57-12/ckpt_2825.pth',\n",
    "}\n",
    "res = []\n",
    "for dataset_environment_str, model_ckpt in models.items():\n",
    "    eval_graphs = load_graphs(dataset_environment_str,\n",
    "                              train_or_eval=\"eval\",\n",
    "                              use_dummy=False,\n",
    "                              max_row=eval_size)\n",
    "\n",
    "    eval_ds = init_dataset(eval_graphs)\n",
    "    scalers = load_scalers_pkl(dataset_environment_str, method_prefix, 'train',\n",
    "                               'Standard')\n",
    "    # op_feature_scaler, y_scaler = scalers\n",
    "    preprocessed_eval_ds = preprocess_dataset(eval_ds)\n",
    "    model = torch.load(model_ckpt)\n",
    "    model.eval()\n",
    "    ds = preprocessed_eval_ds\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    input_batchs, output_batchs = list(), list()\n",
    "    for data in dl:\n",
    "\n",
    "        features, labels = data\n",
    "        features, labels = to_device(\"cuda\", features, labels)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "        input_batchs.append(features)\n",
    "        output_batchs.append(outputs)\n",
    "    y_hat, y = compute_durations(\n",
    "        input_batchs, output_batchs, scalers, eval_graphs)\n",
    "    rand_y, rand_y_hat = [], []\n",
    "    models_y, models_y_hat = [], []\n",
    "    for i in range(len(eval_graphs)):\n",
    "        graph = eval_graphs[i]\n",
    "        if 'rand' in graph.ID:\n",
    "            rand_y.append(y[i])\n",
    "            rand_y_hat.append(y_hat[i])\n",
    "        else:\n",
    "            models_y.append(y[i])\n",
    "            models_y_hat.append(y_hat[i])\n",
    "    res.append(\n",
    "        {\n",
    "            'dataset': dataset_environment_str,\n",
    "            'mre': MetricUtil.mre(y, y_hat),\n",
    "            'rmse': MetricUtil.rmse(y, y_hat),\n",
    "            'rand_mre': MetricUtil.mre(rand_y, rand_y_hat),\n",
    "            'rand_rmse': MetricUtil.rmse(rand_y, rand_y_hat),\n",
    "            'models_mre': MetricUtil.mre(models_y, models_y_hat),\n",
    "            'models_rmse': MetricUtil.rmse(models_y, models_y_hat),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.DataFrame(res)\n",
    "df.to_csv(os.path.join('/root/guohao/DLT-perf-model/notebooks/exp/total_compare',\n",
    "          f'grouping.result.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train process\n",
    "# envs = ['T4_CPUALL', 'P4_CPUALL', 'RTX2080Ti_CPUALL', 'RTX3080Ti_CPUALL',\n",
    "#         'T4_CPU100', 'P4_CPU100', 'RTX2080Ti_CPU100', 'T4_CPU80', 'P4_CPU80', 'RTX2080Ti_CPU80']\n",
    "envs = ['T4_CPUALL', 'P4_CPUALL', 'RTX2080Ti_CPUALL', 'RTX3080Ti_CPUALL',]\n",
    "for env in envs:\n",
    "    train_configs = {\n",
    "        ModelType.MLPTestGrouping.name: Config.from_dict({\n",
    "            \"model\": \"MLPTestGrouping\",\n",
    "            \"all_seed\": 42,\n",
    "            \"dataset_environment_str\": env,\n",
    "            \"dataset_subgraph_grouping_count\": 20,\n",
    "            \"dataset_params\": {\n",
    "                \"duration_summed\": False,\n",
    "            },\n",
    "            \"dataset_dummy\": False,\n",
    "            \"batch_size\": 128,\n",
    "            \"eval_steps\": 5000,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"epochs\": 200,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"meta_configs\": {\n",
    "                \"learning_rate\": 0.005,\n",
    "                \"meta_learning_rate\": 0.001,\n",
    "                \"meta_train_steps\": 1000,\n",
    "                \"meta_fast_adaption_step\": 5,\n",
    "                \"meta_dataset_train_environment_strs\": [dataset_environment_str],\n",
    "                \"meta_dataset_eval_environment_strs\": [dataset_environment_str],\n",
    "            },\n",
    "        }),\n",
    "        ModelType.GCNGrouping.name: Config.from_dict({\n",
    "            \"model\": \"GCNGrouping\",\n",
    "            \"dataset_environment_str\": env,\n",
    "            \"dataset_subgraph_grouping_count\": 25,\n",
    "            \"all_seed\": 42,\n",
    "            \"dataset_params\": {\n",
    "                \"duration_summed\": False,\n",
    "            },\n",
    "            \"dataset_dummy\": False,\n",
    "            \"batch_size\": 32,\n",
    "            \"eval_steps\": 25,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"epochs\": 100,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"meta_configs\": {\n",
    "                \"learning_rate\": 0.005,\n",
    "                \"meta_learning_rate\": 0.001,\n",
    "                \"meta_train_steps\": 1000,\n",
    "                \"meta_fast_adaption_step\": 5,\n",
    "                \"meta_dataset_train_environment_strs\": [dataset_environment_str],\n",
    "                \"meta_dataset_eval_environment_strs\": [dataset_environment_str],\n",
    "            },\n",
    "        }),\n",
    "    }\n",
    "    eval_graphs = load_graphs(env,\n",
    "                              train_or_eval=\"eval\",\n",
    "                              use_dummy=dummy,\n",
    "                              max_row=200_000)\n",
    "    train_graphs = load_graphs(env,\n",
    "                               train_or_eval=\"train\",\n",
    "                               use_dummy=dummy,\n",
    "                               max_row=1000_000)\n",
    "    train_ds = init_dataset(train_graphs)\n",
    "    eval_ds = init_dataset(eval_graphs)\n",
    "    scalers = get_scalers(train_ds)\n",
    "    save_scalers_pkl(scalers, env, method_prefix, 'train',\n",
    "                     conf.dataset_normalization)\n",
    "    # op_feature_scaler, y_scaler = scalers\n",
    "\n",
    "    preprocessed_train_ds = preprocess_dataset(train_ds)\n",
    "    preprocessed_eval_ds = preprocess_dataset(eval_ds)\n",
    "\n",
    "    model_type = ModelType.GCNGrouping\n",
    "    conf = train_configs[model_type.name]\n",
    "\n",
    "    model = init_GCNGrouping_model(preprocessed_train_ds)\n",
    "    model = model.to(conf.device)\n",
    "    single_train_loop(model_type, conf, preprocessed_train_ds,\n",
    "                      preprocessed_eval_ds, model, compute_evaluate_metrics, to_device, suffix=\"grouping\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT-perf-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
