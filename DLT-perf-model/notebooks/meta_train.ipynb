{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from itertools import count\n",
    "from typing import List, Dict\n",
    "from typing import Tuple, Any\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import MSELoss, LSTM, GRU, RNN, L1Loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from learn2learn.data import InfiniteIterator\n",
    "from torch.utils.data import DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import init_logging\n",
    "from base_module import MModule\n",
    "from data import MDataset, Graph, load_graphs\n",
    "from importlib import reload\n",
    "from config import Config\n",
    "import config\n",
    "from data import MDataset, Graph, GraphNode, load_graphs, save_dataset_pkl, load_dataset_pkl, save_scalers_pkl, load_scalers_pkl\n",
    "import data\n",
    "from base_module import MModule, pad_np_vectors\n",
    "import base_module\n",
    "from executor import meta_train, nested_detach, grid_search_loop\n",
    "import executor\n",
    "from objects import ModelType\n",
    "import objects\n",
    "from metric import MetricUtil\n",
    "import metric\n",
    "from logger import init_logging, logging\n",
    "import logger\n",
    "import gcn\n",
    "from gcn import GCNLayer\n",
    "import transformer\n",
    "from transformer import TransformerModel\n",
    "reload(config)\n",
    "reload(data)\n",
    "reload(base_module)\n",
    "reload(executor)\n",
    "reload(objects)\n",
    "reload(metric)\n",
    "reload(logger)\n",
    "reload(gcn)\n",
    "reload(transformer)\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_cls = StandardScaler  # MinMaxScaler\n",
    "dummy = False\n",
    "train_support_maxrow = 500_000\n",
    "train_query_maxrow = 100_000\n",
    "eval_support_maxrow = 500_000  # adaption大小可以收缩\n",
    "eval_query_maxrow = 200_000  # 测试集大小应该和不用元学习一致\n",
    "model_type = ModelType.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = {\n",
    "    ModelType.LSTM.name: Config.from_dict({\n",
    "        \"model\": \"LSTM\",\n",
    "        \"dataset_environment_str\": 'T4_CPUALL',\n",
    "        \"meta_dataset_environment_strs\": ['P4_CPUALL', 'P4_CPU100'],\n",
    "        \"dataset_subgraph_node_size\": 12,\n",
    "        \"dataset_subgraph_step\": 3,\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"model_params\": {\n",
    "            \"num_layers\": 4,\n",
    "            \"bidirectional\": True\n",
    "        },\n",
    "        \"dataset_dummy\": False,\n",
    "        \"batch_size\": 128,\n",
    "        \"eval_steps\": 100,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 1700,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.003,  # LSTM learning rate\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,  # num iterations\n",
    "            \"meta_task_per_step\": 32,  # meta batch_size\n",
    "            \"meta_fast_adaption_step\": 3,  # adapt steps\n",
    "            \"meta_shots\": 5,  # num of samples\n",
    "            \"meta_dataset_train_environment_strs\": ['T4_CPUALL', 'RTX3080Ti_CPUALL', 'T4_CPU100', 'RTX2080Ti_CPU100', 'P4_CPU100'],\n",
    "            \"meta_dataset_eval_environment_strs\": ['P4_CPUALL'],\n",
    "        },\n",
    "    }),\n",
    "}\n",
    "\n",
    "conf: Config = train_configs[model_type.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_features(graph: Graph, subgraph_node_size: int = 10, step: int = 5, dataset_params: Dict = {}) -> \\\n",
    "        Tuple[List[Dict], List[Dict]]:\n",
    "    subgraphs, _ = graph.subgraphs(\n",
    "        subgraph_node_size=subgraph_node_size, step=step)\n",
    "    X, Y = list(), list()\n",
    "\n",
    "    def subgraph_feature(nodes: List[GraphNode]):\n",
    "        feature_matrix = list()\n",
    "        for node in nodes:\n",
    "            feature = node.op.to_feature_array(\n",
    "                mode=dataset_params.get(\"mode\", \"complex\"))\n",
    "            feature = np.array(feature)\n",
    "            feature_matrix.append(feature)\n",
    "\n",
    "        feature_matrix = pad_np_vectors(feature_matrix)\n",
    "        feature_matrix = np.array(feature_matrix)\n",
    "\n",
    "        adj_matrix = [\n",
    "            [0.] * len(nodes) for _ in range(len(nodes))\n",
    "        ]\n",
    "        for curr_idx, node in enumerate(nodes):\n",
    "            if curr_idx + 1 < len(nodes):\n",
    "                adj_matrix[curr_idx][curr_idx+1] = 1.\n",
    "\n",
    "        adj_matrix = np.array(adj_matrix)\n",
    "        # x\n",
    "        feature = {\n",
    "            \"x_graph_id\": graph.ID,\n",
    "            \"x_node_ids\": \"|\".join([str(node.node_id) for node in nodes]),\n",
    "            \"x_subgraph_feature\": feature_matrix,\n",
    "            \"x_adj_matrix\": adj_matrix\n",
    "        }\n",
    "\n",
    "        # y\n",
    "        subgraph_duration = sum(node.duration + node.gap for node in subgraph)\n",
    "        nodes_durations = list()\n",
    "        for node in subgraph:\n",
    "            node_duration_label = (\n",
    "                node.duration, node.gap\n",
    "            )\n",
    "            nodes_durations.append(node_duration_label)\n",
    "\n",
    "        label = {\n",
    "            \"y_graph_id\": graph.ID,\n",
    "            \"y_nodes_durations\": nodes_durations,\n",
    "            \"y_subgraph_durations\": (subgraph_duration,)\n",
    "        }\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    for i, subgraph in enumerate(subgraphs):\n",
    "        x, y = subgraph_feature(subgraph)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def init_dataset(graphs: List[Graph]) -> MDataset:\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    subgraph_feature_maxsize = 0\n",
    "\n",
    "    for graph in graphs:\n",
    "        X_, Y_ = subgraph_features(graph=graph,\n",
    "                                   subgraph_node_size=conf.dataset_subgraph_node_size,\n",
    "                                   step=conf.dataset_subgraph_step,\n",
    "                                   dataset_params=conf.dataset_params)\n",
    "        for x in X_:\n",
    "            subgraph_feature_size = len(x[\"x_subgraph_feature\"][0])\n",
    "            subgraph_feature_maxsize = max(\n",
    "                subgraph_feature_maxsize, subgraph_feature_size)\n",
    "\n",
    "        X.extend(X_)\n",
    "        Y.extend(Y_)\n",
    "\n",
    "    for x in X:\n",
    "        x[\"x_subgraph_feature\"] = pad_np_vectors(\n",
    "            x[\"x_subgraph_feature\"], maxsize=subgraph_feature_maxsize)\n",
    "\n",
    "    dataset = MDataset(X, Y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(ds: MDataset, scalers) -> MDataset:\n",
    "    x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "\n",
    "    processed_features = list()\n",
    "    processed_labels = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "        assert isinstance(x_subgraph_feature, list)\n",
    "        x_subgraph_feature = np.array(x_subgraph_feature).astype(np.float32)\n",
    "        transformed_x_subgraph_feature = x_subgraph_feature_scaler.transform(\n",
    "            x_subgraph_feature)\n",
    "\n",
    "        x_adj_matrix = feature[\"x_adj_matrix\"]\n",
    "        x_adj_matrix = np.array(x_adj_matrix).astype(np.float32)\n",
    "\n",
    "        y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "        assert isinstance(y_nodes_durations, list)\n",
    "        y_nodes_durations = np.array(y_nodes_durations).astype(np.float32)\n",
    "        transformed_y_nodes_durations = y_nodes_durations_scaler.transform(\n",
    "            y_nodes_durations)\n",
    "\n",
    "        y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "        y_subgraph_durations_array = (y_subgraph_durations,)\n",
    "        y_subgraph_durations_array = y_subgraph_durations_scaler.transform(\n",
    "            y_subgraph_durations_array)\n",
    "        transformed_y_subgraph_durations = y_subgraph_durations_array[0]\n",
    "\n",
    "        processed_features.append({\n",
    "            \"x_graph_id\": feature[\"x_graph_id\"],\n",
    "            \"x_node_ids\": feature[\"x_node_ids\"],\n",
    "            \"x_subgraph_feature\": torch.Tensor(transformed_x_subgraph_feature),\n",
    "            \"x_adj_matrix\": torch.Tensor(x_adj_matrix)\n",
    "        })\n",
    "\n",
    "        processed_labels.append({\n",
    "            \"y_graph_id\": label[\"y_graph_id\"],\n",
    "            \"y_nodes_durations\": torch.Tensor(transformed_y_nodes_durations),\n",
    "            \"y_subgraph_durations\": torch.Tensor(transformed_y_subgraph_durations)\n",
    "        })\n",
    "\n",
    "    ds = MDataset(processed_features, processed_labels)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalers(raw_train_ds: MDataset):\n",
    "\n",
    "    def _preprocess_required_data(ds: MDataset):\n",
    "        x_subgraph_feature_array = list()\n",
    "        y_nodes_durations_array = list()\n",
    "        y_subgraph_durations_array = list()\n",
    "\n",
    "        for data in ds:\n",
    "            feature, label = data\n",
    "            x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "            assert isinstance(x_subgraph_feature, list)\n",
    "            x_subgraph_feature_array.extend(x_subgraph_feature)\n",
    "\n",
    "            y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "            assert isinstance(y_nodes_durations, list)\n",
    "            y_nodes_durations_array.extend(y_nodes_durations)\n",
    "\n",
    "            y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "            y_subgraph_durations_array.append(y_subgraph_durations)\n",
    "\n",
    "        x_subgraph_feature_array = np.array(x_subgraph_feature_array)\n",
    "        y_nodes_durations_array = np.array(y_nodes_durations_array)\n",
    "        y_subgraph_durations_array = np.array(y_subgraph_durations_array)\n",
    "        return [x_subgraph_feature_array, y_nodes_durations_array, y_subgraph_durations_array]\n",
    "\n",
    "    scaler_cls = conf.dataset_normalizer_cls\n",
    "\n",
    "    x_subgraph_feature_array, y_nodes_durations_array, y_subgraph_durations_array = _preprocess_required_data(\n",
    "        ds=raw_train_ds)\n",
    "\n",
    "    x_subgraph_feature_scaler = scaler_cls()\n",
    "    x_subgraph_feature_scaler.fit(x_subgraph_feature_array)\n",
    "\n",
    "    y_nodes_durations_scaler = scaler_cls()\n",
    "    y_nodes_durations_scaler.fit(y_nodes_durations_array)\n",
    "\n",
    "    y_subgraph_durations_scaler = scaler_cls()\n",
    "    y_subgraph_durations_scaler.fit(y_subgraph_durations_array)\n",
    "\n",
    "    return x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_evaluate_metrics(input_batches, output_batches, eval_loss, env) -> Dict[str, float]:\n",
    "\n",
    "    def compute_graph_nodes_durations(outputs_, node_ids_str_, env):\n",
    "        x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = meta_eval_scalers[\n",
    "            env]\n",
    "        node_to_durations = defaultdict(list)\n",
    "        for i, output_ in enumerate(outputs_):\n",
    "            node_ids = node_ids_str_[i]\n",
    "            node_ids_ = node_ids.split(\"|\")\n",
    "            assert len(output_) == len(node_ids_)\n",
    "            transformed: np.ndarray = y_nodes_durations_scaler.inverse_transform(\n",
    "                output_)\n",
    "            for i, node_id in enumerate(node_ids_):\n",
    "                node_to_durations[node_id].append(np.sum(transformed[i]))\n",
    "        node_to_duration = {k: np.average(v)\n",
    "                            for k, v in node_to_durations.items()}\n",
    "        return node_to_duration\n",
    "\n",
    "    graph_id_to_node_to_duration = defaultdict(lambda: defaultdict(list))\n",
    "    for inputs, outputs in zip(input_batches, output_batches):\n",
    "        outputs = nested_detach(outputs)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        graph_groups = defaultdict(list)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            graph_groups[graph_id].append(i)\n",
    "\n",
    "        for graph_id, indices in graph_groups.items():\n",
    "            group_x_node_ids = [v for i, v in enumerate(\n",
    "                inputs[\"x_node_ids\"]) if i in indices]\n",
    "            group_outputs = [v for i, v in enumerate(outputs) if i in indices]\n",
    "            node_to_durations = compute_graph_nodes_durations(\n",
    "                group_outputs, group_x_node_ids, env)\n",
    "            for node, duration in node_to_durations.items():\n",
    "                graph_id_to_node_to_duration[graph_id][node].append(duration)\n",
    "    graph_id_to_duration_pred = dict()\n",
    "    # TODO check this!!!\n",
    "    for graph_id, node_to_duration in graph_id_to_node_to_duration.items():\n",
    "        duration_pred = 0\n",
    "        for _, duration_preds in node_to_duration.items():\n",
    "            duration_pred += np.average(duration_preds)\n",
    "        graph_id_to_duration_pred[graph_id] = duration_pred\n",
    "    duration_metrics = MetricUtil.compute_duration_metrics(\n",
    "        meta_eval_query_graphs[env], graph_id_to_duration_pred)\n",
    "    return {\"eval_loss\": eval_loss, **duration_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(conf: Config, features, labels):\n",
    "    features['x_subgraph_feature'] = features['x_subgraph_feature'].to(\n",
    "        conf.device)\n",
    "    features['x_adj_matrix'] = features['x_adj_matrix'].to(conf.device)\n",
    "    labels['y_nodes_durations'] = labels['y_nodes_durations'].to(conf.device)\n",
    "    labels['y_subgraph_durations'] = labels['y_subgraph_durations'].to(\n",
    "        conf.device)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_support_graphs: Dict[str, List[Graph]] = dict()\n",
    "meta_train_query_graphs: Dict[str, List[Graph]] = dict()\n",
    "meta_eval_support_graphs: Dict[str, List[Graph]] = dict()  # 用于adapt\n",
    "meta_eval_query_graphs: Dict[str, List[Graph]] = dict()  # 用于test\n",
    "\n",
    "for env_str in conf.meta_dataset_train_environment_strs:\n",
    "    meta_train_support_graphs[env_str] = load_graphs(\n",
    "        env_str, train_or_eval='train', use_dummy=dummy, max_row=train_support_maxrow)\n",
    "for env_str in conf.meta_dataset_train_environment_strs:\n",
    "    meta_train_query_graphs[env_str] = load_graphs(\n",
    "        env_str, train_or_eval='eval', use_dummy=dummy, max_row=train_query_maxrow)\n",
    "for env_str in conf.meta_dataset_eval_environment_strs:\n",
    "    meta_eval_support_graphs[env_str] = load_graphs(\n",
    "        env_str, train_or_eval='train', use_dummy=dummy, max_row=eval_support_maxrow)\n",
    "for env_str in conf.meta_dataset_eval_environment_strs:\n",
    "    meta_eval_query_graphs[env_str] = load_graphs(\n",
    "        env_str, train_or_eval='eval', use_dummy=dummy, max_row=eval_query_maxrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_scalers = {}\n",
    "meta_eval_scalers = {}\n",
    "for env_str, graphs in meta_train_support_graphs.items():\n",
    "    meta_train_scalers[env_str] = get_scalers(init_dataset(graphs))\n",
    "for env_str, graphs in meta_eval_support_graphs.items():\n",
    "    meta_eval_scalers[env_str] = get_scalers(init_dataset(graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_preprocessed_train_support_dss = {}\n",
    "meta_preprocessed_eval_support_dss = {}\n",
    "meta_preprocessed_train_query_dss = {}\n",
    "meta_preprocessed_eval_query_dss = {}\n",
    "meta_preporcessed_train_support_loaders = {}\n",
    "meta_preprocessed_eval_support_loaders = {}\n",
    "meta_preprocessed_train_query_loaders = {}\n",
    "meta_preprocessed_eval_query_loaders = {}\n",
    "\n",
    "for env_str, graphs in meta_train_support_graphs.items():\n",
    "    train_ds = init_dataset(graphs)\n",
    "    preprocessed_train_ds = preprocess_dataset(\n",
    "        train_ds, meta_train_scalers[env_str])\n",
    "    meta_preprocessed_train_support_dss[env_str] = preprocessed_train_ds\n",
    "    sampler = RandomSampler(\n",
    "        meta_preprocessed_train_support_dss[env_str], replacement=True)\n",
    "    meta_preporcessed_train_support_loaders[env_str] = InfiniteIterator(\n",
    "        DataLoader(meta_preprocessed_train_support_dss[env_str], sampler=sampler, batch_size=conf.meta_shots))\n",
    "\n",
    "\n",
    "for env_str, graphs in meta_eval_support_graphs.items():\n",
    "    eval_ds = init_dataset(graphs)\n",
    "\n",
    "    meta_preprocessed_eval_support_dss[env_str] = preprocess_dataset(\n",
    "        eval_ds, meta_eval_scalers[env_str])\n",
    "    sampler = RandomSampler(\n",
    "        meta_preprocessed_eval_support_dss[env_str], replacement=True)\n",
    "    meta_preprocessed_eval_support_loaders[env_str] = InfiniteIterator(\n",
    "        DataLoader(meta_preprocessed_eval_support_dss[env_str], sampler=sampler, batch_size=conf.meta_shots))\n",
    "\n",
    "for env_str, graphs in meta_train_query_graphs.items():\n",
    "    train_ds = init_dataset(graphs)\n",
    "    meta_preprocessed_train_query_dss[env_str] = preprocess_dataset(\n",
    "        train_ds, meta_train_scalers[env_str])\n",
    "    sampler = RandomSampler(\n",
    "        meta_preprocessed_train_query_dss[env_str], replacement=True)\n",
    "    meta_preprocessed_train_query_loaders[env_str] = InfiniteIterator(\n",
    "        DataLoader(meta_preprocessed_train_query_dss[env_str], sampler=sampler, batch_size=conf.meta_shots))\n",
    "\n",
    "for env_str, graphs in meta_eval_query_graphs.items():\n",
    "    eval_ds = init_dataset(graphs)\n",
    "    meta_preprocessed_eval_query_dss[env_str] = preprocess_dataset(\n",
    "        eval_ds, meta_eval_scalers[env_str])\n",
    "    sampler = RandomSampler(\n",
    "        meta_preprocessed_eval_query_dss[env_str], replacement=True)\n",
    "    meta_preprocessed_eval_query_loaders[env_str] = InfiniteIterator(\n",
    "        DataLoader(meta_preprocessed_eval_query_dss[env_str], sampler=sampler, batch_size=conf.meta_shots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(MModule):\n",
    "    def __init__(self, feature_size, nodes_durations_len, num_layers, bidirectional, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = LSTM(input_size=feature_size, hidden_size=feature_size, num_layers=num_layers, batch_first=True,\n",
    "                         bidirectional=bidirectional)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.project = torch.nn.Linear(\n",
    "            in_features=feature_size * num_directions, out_features=nodes_durations_len)\n",
    "        self.loss_fn = L1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_search_model_params() -> Dict[str, List[Any]]:\n",
    "        return {\n",
    "            \"num_layers\": [4, 6, 8],\n",
    "            \"bidirectional\": [True, False],\n",
    "            \"learning_rate\": [1e-4, 1e-5],\n",
    "            'batch_size': [32, 64],\n",
    "            'epochs': [20],\n",
    "            'optimizer': ['Adam', 'SGD'],\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_subgraph_feature\"]\n",
    "        out, _ = self.lstm(X)\n",
    "        Y = self.project(out)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_nodes_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_LSTM_model() -> MModule | Any:\n",
    "    def default_model_params() -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"num_layers\": 4,\n",
    "            \"bidirectional\": True,\n",
    "        }\n",
    "\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    x_node_feature_size = len(sample_x_dict[\"x_subgraph_feature\"][0])\n",
    "    y_nodes_durations_len = len(sample_y_dict[\"y_nodes_durations\"][0])\n",
    "    model_params = conf.model_params\n",
    "    final_params = default_model_params()\n",
    "    for k, v in final_params.items():\n",
    "        final_params[k] = model_params.get(k, v)\n",
    "    print(final_params)\n",
    "    return LSTMModel(\n",
    "        feature_size=x_node_feature_size,\n",
    "        nodes_durations_len=y_nodes_durations_len,\n",
    "        **final_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model_funcs = {\n",
    "    ModelType.LSTM.name: init_LSTM_model,\n",
    "}\n",
    "\n",
    "model_type = ModelType.LSTM\n",
    "conf: Config = train_configs[model_type.name]\n",
    "init_model = init_model_funcs[model_type.name]\n",
    "model = init_model()\n",
    "model = model.to(conf.device)\n",
    "meta_train(model_type, conf, model,\n",
    "           meta_preporcessed_train_support_loaders,\n",
    "           meta_preprocessed_train_query_loaders,\n",
    "           meta_preprocessed_eval_support_loaders,\n",
    "           meta_preprocessed_eval_query_loaders,\n",
    "           meta_preprocessed_eval_query_dss,\n",
    "           compute_evaluate_metrics, to_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT-pref-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
